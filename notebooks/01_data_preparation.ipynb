{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Retail GenAI Multi-Modal System\n",
    "\n",
    "This notebook demonstrates how to gather, clean, and prepare retail data for a multi-modal GenAI system. We'll work with:\n",
    "\n",
    "1. Product images (for computer vision)\n",
    "2. Product descriptions (for text understanding)\n",
    "3. Sales information (for recommendation engine)\n",
    "4. Store layout data (for spatial context)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Retail data presents unique challenges:\n",
    "* High variance in product appearances\n",
    "* Domain-specific terminology \n",
    "* Seasonal and trend-dependent patterns\n",
    "* Multi-modal nature (text, images, structured data)\n",
    "\n",
    "This notebook provides a systematic approach to preparing this data for AI model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's ensure we have all the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies if needed\n",
    "!pip install -q pandas numpy torch torchvision pillow matplotlib tqdm albumentations opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For GPU detection\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "For this demo, we'll use a combination of:\n",
    "- Sample retail dataset (included in the repository)\n",
    "- Programmatic generation of additional data\n",
    "- Optional: Connection to public retail datasets\n",
    "\n",
    "Let's first examine the sample data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths to example data\n",
    "REPO_ROOT = Path(\"..\")\n",
    "DATA_DIR = REPO_ROOT / \"examples\" / \"product_data\"\n",
    "IMAGES_DIR = REPO_ROOT / \"examples\" / \"images\"\n",
    "\n",
    "# Check if example data exists, otherwise download it\n",
    "if not DATA_DIR.exists() or len(list(DATA_DIR.glob(\"*.csv\"))) == 0:\n",
    "    print(\"Example data not found. Downloading...\")\n",
    "    # This would be implemented in the actual notebook\n",
    "    # !python ../src/utils/download_demo_data.py\n",
    "    print(\"For this demo, we'll create some sample data\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "    \n",
    "    # Generate sample product catalog data\n",
    "    product_data = {\n",
    "        \"product_id\": list(range(1, 101)),\n",
    "        \"name\": [f\"Product {i}\" for i in range(1, 101)],\n",
    "        \"category\": np.random.choice([\"Electronics\", \"Clothing\", \"Groceries\", \"Home\", \"Beauty\"], 100),\n",
    "        \"price\": np.random.uniform(5, 500, 100).round(2),\n",
    "        \"description\": [f\"This is a detailed description of product {i}\" for i in range(1, 101)],\n",
    "        \"in_stock\": np.random.choice([True, False], 100, p=[0.8, 0.2])\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    product_df.to_csv(DATA_DIR / \"product_catalog.csv\", index=False)\n",
    "    \n",
    "    # Print sample of generated data\n",
    "    print(\"Sample data generated successfully!\")\n",
    "    print(product_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading Product Catalog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load product catalog\n",
    "product_catalog = pd.read_csv(DATA_DIR / \"product_catalog.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Total products: {len(product_catalog)}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(product_catalog['category'].value_counts())\n",
    "\n",
    "# Display a sample of products\n",
    "product_catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Image Processing Pipeline\n",
    "\n",
    "For a retail AI system, we need to process product images for both:\n",
    "- Training our computer vision models\n",
    "- Creating multi-modal embeddings\n",
    "\n",
    "Let's build a pipeline to handle these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define image transformation pipeline\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0)),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:  # validation/test transforms\n",
    "        return A.Compose([\n",
    "            A.Resize(height=256, width=256),\n",
    "            A.CenterCrop(height=224, width=224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# Function to process images with GPU acceleration when available\n",
    "def process_image_batch(image_paths, transforms, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    processed_images = []\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        # Read image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed = transforms(image=img)\n",
    "        tensor_img = transformed[\"image\"]\n",
    "        \n",
    "        # Move to device\n",
    "        tensor_img = tensor_img.to(device)\n",
    "        processed_images.append(tensor_img)\n",
    "    \n",
    "    # Stack into a batch\n",
    "    if processed_images:\n",
    "        return torch.stack(processed_images)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Text Processing Pipeline\n",
    "\n",
    "Next, we'll create a pipeline for processing product descriptions and other text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# We'll use the transformers library for text processing\n",
    "!pip install -q transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model for text embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to create text embeddings\n",
    "def process_text_batch(texts, tokenizer, model, device=None, max_length=128):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encoded = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Preprocessing\n",
    "\n",
    "Now that we have our basic pipelines, let's preprocess our dataset for the multi-modal fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This section would contain:\n",
    "# 1. Data cleaning (handling missing values, duplicates, etc.)\n",
    "# 2. Feature engineering (creating new features from existing data)\n",
    "# 3. Data splitting (training, validation, test sets)\n",
    "# 4. Creating combined representations\n",
    "\n",
    "# For brevity, we'll outline the key steps with sample code\n",
    "\n",
    "# 1. Data cleaning\n",
    "print(\"Checking for missing values...\")\n",
    "print(product_catalog.isnull().sum())\n",
    "\n",
    "# Fill missing descriptions with a default message\n",
    "product_catalog['description'] = product_catalog['description'].fillna('No description available')\n",
    "\n",
    "# 2. Feature engineering - Create rich text representation\n",
    "product_catalog['full_text'] = (\n",
    "    'Product: ' + product_catalog['name'] + '. ' +\n",
    "    'Category: ' + product_catalog['category'] + '. ' +\n",
    "    'Price: $' + product_catalog['price'].astype(str) + '. ' +\n",
    "    'Description: ' + product_catalog['description'] + '. ' +\n",
    "    'In stock: ' + product_catalog['in_stock'].map({True: 'Yes', False: 'No'})\n",
    ")\n",
    "\n",
    "# Display a sample of the enriched text\n",
    "print(\"\\nSample of enriched product text:\")\n",
    "print(product_catalog['full_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing Image-Text Pairs\n",
    "\n",
    "For multi-modal training, we need to pair product images with their textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This would be implemented in the final notebook\n",
    "# For now, we'll demonstrate with pseudocode:\n",
    "\n",
    "'''\n",
    "# Create a mapping of product_id to image path\n",
    "image_mapping = {}\n",
    "for img_path in IMAGES_DIR.glob(\"*.jpg\"):\n",
    "    # Assuming image filename format: product_{id}.jpg\n",
    "    product_id = int(img_path.stem.split('_')[1])\n",
    "    image_mapping[product_id] = img_path\n",
    "\n",
    "# Add image paths to the product catalog\n",
    "product_catalog['image_path'] = product_catalog['product_id'].map(image_mapping)\n",
    "\n",
    "# Filter to only products with images\n",
    "products_with_images = product_catalog.dropna(subset=['image_path'])\n",
    "print(f\"Products with images: {len(products_with_images)} out of {len(product_catalog)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Export and Integration\n",
    "\n",
    "Finally, we'll export our processed data for use in the model building notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create processed data directory\n",
    "PROCESSED_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Export the processed catalog\n",
    "product_catalog.to_csv(PROCESSED_DIR / \"processed_product_catalog.csv\", index=False)\n",
    "\n",
    "print(f\"Processed data saved to: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NVIDIA GPU Acceleration Benchmarks\n",
    "\n",
    "Let's demonstrate the performance advantages of using NVIDIA GPUs for our data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def benchmark_processing(num_samples=100):\n",
    "    # Generate dummy text data\n",
    "    dummy_texts = [\n",
    "        f\"This is a sample product description for product {i}.\" for i in range(num_samples)\n",
    "    ]\n",
    "    \n",
    "    # Load model for benchmarking\n",
    "    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Benchmark on CPU\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.to(cpu_device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    _ = process_text_batch(dummy_texts, tokenizer, model, device=cpu_device)\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark on GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_device = torch.device(\"cuda\")\n",
    "        model.to(gpu_device)\n",
    "        \n",
    "        # Warm-up run\n",
    "        _ = process_text_batch(dummy_texts[:10], tokenizer, model, device=gpu_device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = process_text_batch(dummy_texts, tokenizer, model, device=gpu_device)\n",
    "        gpu_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"CPU time: {cpu_time:.2f} seconds\")\n",
    "        print(f\"GPU time: {gpu_time:.2f} seconds\")\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.1f}x\")\n",
    "    else:\n",
    "        print(f\"CPU time: {cpu_time:.2f} seconds\")\n",
    "        print(\"GPU not available for comparison\")\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Benchmarking text processing performance...\")\n",
    "benchmark_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Set up the environment for retail data processing\n",
    "2. Created data processing pipelines for images and text\n",
    "3. Prepared and exported the processed data\n",
    "4. Demonstrated GPU acceleration benefits\n",
    "\n",
    "In the next notebook, we'll build multi-modal models using this processed data to create a unified retail AI system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
