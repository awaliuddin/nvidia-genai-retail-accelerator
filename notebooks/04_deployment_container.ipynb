{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerization and Deployment of Retail GenAI System\n",
    "\n",
    "This notebook demonstrates how to containerize and deploy the multi-modal retail GenAI system for production environments. We'll cover:\n",
    "\n",
    "1. Packaging the model and code into Docker containers\n",
    "2. Setting up GPU-accelerated containers with NVIDIA Container Toolkit\n",
    "3. Deploying with Docker Compose for development\n",
    "4. Kubernetes deployment for production\n",
    "5. Performance tuning and monitoring\n",
    "\n",
    "This approach ensures the system can be easily deployed across various environments while leveraging NVIDIA GPUs for maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "Before containerizing our application, we need to ensure we have the necessary tools installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for Docker installation\n",
    "!docker --version\n",
    "\n",
    "# Check for NVIDIA Docker support\n",
    "!docker info | grep -i nvidia\n",
    "\n",
    "# Check for NVIDIA Container Toolkit\n",
    "!nvidia-container-cli --version 2>/dev/null || echo \"NVIDIA Container Toolkit not found\"\n",
    "\n",
    "# Check available GPUs\n",
    "!nvidia-smi 2>/dev/null || echo \"No NVIDIA GPUs detected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Docker with NVIDIA GPU Support\n",
    "\n",
    "The NVIDIA Container Toolkit enables using NVIDIA GPUs in Docker containers. This is essential for deep learning workloads that require GPU acceleration.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **NVIDIA Container Toolkit** (formerly nvidia-docker): Enables GPU access in containers\n",
    "2. **NVIDIA Container Runtime**: Handles GPU allocation to containers\n",
    "3. **NVIDIA Base Images**: CUDA-enabled base images for building ML containers\n",
    "\n",
    "### How it Works:\n",
    "\n",
    "When a container is launched with `--gpus all`, the NVIDIA Container Toolkit:\n",
    "1. Identifies available GPUs\n",
    "2. Mounts necessary drivers\n",
    "3. Sets required environment variables\n",
    "4. Configures CUDA libraries in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reviewing Our Dockerfile\n",
    "\n",
    "Let's review the Dockerfile we created for our retail GenAI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define repository root\n",
    "REPO_ROOT = Path(\"..\")\n",
    "DOCKER_DIR = REPO_ROOT / \"docker\"\n",
    "\n",
    "# Read the Dockerfile\n",
    "with open(DOCKER_DIR / \"Dockerfile\", \"r\") as f:\n",
    "    dockerfile_content = f.read()\n",
    "\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of Our Dockerfile\n",
    "\n",
    "Our Dockerfile is built on top of NVIDIA's PyTorch container, which includes:\n",
    "\n",
    "1. **Base Image**: NVIDIA's optimized PyTorch container with CUDA support\n",
    "2. **Dependencies**: System libraries for OpenCV and other requirements\n",
    "3. **Python Libraries**: Libraries from requirements.txt\n",
    "4. **Application Code**: Our retail GenAI application code\n",
    "\n",
    "The use of NVIDIA's container as a base image ensures that we have all the necessary CUDA libraries and optimizations for PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Docker Compose Configuration\n",
    "\n",
    "Let's look at our Docker Compose configuration which orchestrates multiple containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read the Docker Compose file\n",
    "with open(DOCKER_DIR / \"docker-compose.yml\", \"r\") as f:\n",
    "    docker_compose_content = f.read()\n",
    "\n",
    "print(docker_compose_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of Our Docker Compose Setup\n",
    "\n",
    "Our `docker-compose.yml` file includes:\n",
    "\n",
    "1. **Main Application Container**: Our retail GenAI service with GPU support\n",
    "2. **Vector Database Container**: For storing and searching embeddings\n",
    "3. **Volume Mounts**: For persistent data storage\n",
    "4. **GPU Configuration**: NVIDIA GPU resource allocation\n",
    "5. **Port Mapping**: For accessing the API and interfaces\n",
    "\n",
    "The `deploy` section with `resources.reservations.devices` is critical for GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Docker Image\n",
    "\n",
    "Now let's build the Docker image for our application. This would typically be done in a terminal, but we'll show the commands here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This cell outputs the commands you would run in a terminal\n",
    "# We don't execute them directly as they require Docker privileges\n",
    "\n",
    "print(\"# Navigate to the repository root\")\n",
    "print(f\"cd {REPO_ROOT.absolute()}\")\n",
    "print(\"\\n# Build the Docker image\")\n",
    "print(\"docker build -t retail-genai-accelerator:latest -f docker/Dockerfile .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Build Optimization for ML Models\n",
    "\n",
    "Building efficient Docker images for ML applications requires some special considerations:\n",
    "\n",
    "1. **Layer Caching**: Order dependencies to maximize cache usage\n",
    "2. **Multi-stage Builds**: Separate building from runtime\n",
    "3. **Model Files**: Handle large model files carefully\n",
    "\n",
    "Here's an example of an optimized Dockerfile for ML applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of an optimized Dockerfile (not for execution)\n",
    "optimized_dockerfile = \"\"\"\n",
    "# Build stage\n",
    "FROM nvcr.io/nvidia/pytorch:23.12-py3 AS builder\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt /tmp/\n",
    "RUN pip install --user --no-cache-dir -r /tmp/requirements.txt\n",
    "\n",
    "# Runtime stage\n",
    "FROM nvcr.io/nvidia/pytorch:23.12-py3\n",
    "\n",
    "# Install runtime dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglib2.0-0 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy Python packages from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code\n",
    "COPY src /app/src\n",
    "COPY examples /app/examples\n",
    "COPY models /app/models\n",
    "COPY README.md /app/\n",
    "\n",
    "# Create necessary directories\n",
    "RUN mkdir -p /app/data /app/logs\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONPATH=/app:$PYTHONPATH\n",
    "ENV NVIDIA_VISIBLE_DEVICES=all\n",
    "ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "\n",
    "# Entry point\n",
    "CMD [\"python\", \"-m\", \"src.api.server\"]\n",
    "\"\"\"\n",
    "\n",
    "print(optimized_dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Application with Docker Compose\n",
    "\n",
    "With our Docker image built, we can use Docker Compose to run the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Commands to run the application (not executed)\n",
    "print(\"# Navigate to the repository root\")\n",
    "print(f\"cd {REPO_ROOT.absolute()}\")\n",
    "print(\"\\n# Start the application with Docker Compose\")\n",
    "print(\"docker-compose -f docker/docker-compose.yml up -d\")\n",
    "print(\"\\n# Check the running containers\")\n",
    "print(\"docker-compose -f docker/docker-compose.yml ps\")\n",
    "print(\"\\n# View logs\")\n",
    "print(\"docker-compose -f docker/docker-compose.yml logs -f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NVIDIA Container Toolkit Details\n",
    "\n",
    "Using GPUs in Docker containers requires the NVIDIA Container Toolkit. Here's how to ensure it's properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation steps for NVIDIA Container Toolkit (not executed)\n",
    "nvidia_container_toolkit_install = \"\"\"\n",
    "# Add NVIDIA package repositories\n",
    "distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "\n",
    "# Install nvidia-docker2 package\n",
    "apt-get update\n",
    "apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "# Restart Docker service\n",
    "systemctl restart docker\n",
    "\"\"\"\n",
    "\n",
    "print(\"Installation steps for NVIDIA Container Toolkit:\")\n",
    "print(nvidia_container_toolkit_install)\n",
    "\n",
    "# Test NVIDIA GPU access in Docker (not executed)\n",
    "print(\"\\nTest GPU access with:\")\n",
    "print(\"docker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying GPU Access in Containers\n",
    "\n",
    "After starting our containers, we can verify that our application has access to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify GPU access in the running container (not executed)\n",
    "print(\"# Execute nvidia-smi in the container\")\n",
    "print(\"docker exec retail-genai-accelerator nvidia-smi\")\n",
    "\n",
    "# Check if PyTorch can see the GPUs (not executed)\n",
    "print(\"\\n# Check if PyTorch can access GPUs\")\n",
    "print(\"docker exec retail-genai-accelerator python -c \\\"import torch; print('CUDA available:', torch.cuda.is_available()); print('GPU count:', torch.cuda.device_count()); print('GPU name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating an Inference API Module\n",
    "\n",
    "For production deployment, let's create a dedicated API module. We'll define this in a new file in our repository: `src/api/server.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example API server code (not to be executed directly)\n",
    "api_server_code = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "API server for the Retail GenAI system.\n",
    "\n",
    "This module provides a FastAPI server that exposes the functionality\n",
    "of the Retail GenAI system through RESTful endpoints.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# Ensure the repository root is in the Python path\n",
    "repo_root = Path(__file__).parent.parent.parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.models.multimodal_fusion import RetailProductFusionModel, create_nvidia_optimized_fusion_model\n",
    "from src.inference.pipeline import RetailGenAIPipeline\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"retail_genai_api.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "DEBUG = os.environ.get(\"DEBUG\", \"False\").lower() in (\"true\", \"1\", \"t\")\n",
    "HOST = os.environ.get(\"HOST\", \"0.0.0.0\")\n",
    "PORT = int(os.environ.get(\"PORT\", \"8000\"))\n",
    "MODEL_DIR = os.environ.get(\"MODEL_DIR\", str(repo_root / \"models\"))\n",
    "USE_GPU = os.environ.get(\"USE_GPU\", \"True\").lower() in (\"true\", \"1\", \"t\")\n",
    "\n",
    "# Define API models\n",
    "class ProductQuery(BaseModel):\n",
    "    image_base64: str\n",
    "    text: str\n",
    "\n",
    "class ProductQuestion(BaseModel):\n",
    "    image_base64: str\n",
    "    question: str\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    gpu_available: bool\n",
    "    models_loaded: bool\n",
    "\n",
    "# Initialize the API\n",
    "app = FastAPI(\n",
    "    title=\"Retail GenAI API\",\n",
    "    description=\"Multi-modal API for retail applications powered by NVIDIA GPUs\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # For production, specify your domains\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Global variables for models and pipeline\n",
    "pipeline = None\n",
    "\n",
    "# Load models on startup\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global pipeline\n",
    "    try:\n",
    "        logger.info(\"Initializing models...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "        \n",
    "        # Initialize the pipeline with models\n",
    "        pipeline = RetailGenAIPipeline.from_pretrained(\n",
    "            model_dir=MODEL_DIR,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Models initialized successfully. Using device: {device}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing models: {e}\")\n",
    "        raise\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    gpu_available = torch.cuda.is_available() and USE_GPU\n",
    "    models_loaded = pipeline is not None\n",
    "    \n",
    "    if not models_loaded:\n",
    "        status = \"warning\"\n",
    "    else:\n",
    "        status = \"ok\"\n",
    "    \n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"models_loaded\": models_loaded\n",
    "    }\n",
    "\n",
    "# Product classification endpoint\n",
    "@app.post(\"/predict\")\n",
    "async def predict_product(query: ProductQuery):\n",
    "    if pipeline is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Models not initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_data = base64.b64decode(query.image_base64)\n",
    "        image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Run prediction\n",
    "        start_time = time.time()\n",
    "        result = pipeline.predict(image, query.text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Add processing time\n",
    "        result[\"processing_time\"] = processing_time\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in prediction: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Product Q&A endpoint\n",
    "@app.post(\"/answer\")\n",
    "async def answer_question(query: ProductQuestion):\n",
    "    if pipeline is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Models not initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_data = base64.b64decode(query.image_base64)\n",
    "        image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Answer question\n",
    "        start_time = time.time()\n",
    "        result = pipeline.answer_product_question(image, query.question)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Add processing time\n",
    "        result[\"processing_time\"] = processing_time\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in answering question: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Shelf analysis endpoint\n",
    "@app.post(\"/analyze_shelf\")\n",
    "async def analyze_shelf(image: UploadFile = File(...)):\n",
    "    if pipeline is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Models not initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Read image\n",
    "        image_data = await image.read()\n",
    "        pil_image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Process shelf image\n",
    "        start_time = time.time()\n",
    "        result = pipeline.process_shelf_image(pil_image)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Add processing time\n",
    "        result[\"processing_time\"] = processing_time\n",
    "        \n",
    "        # Create visualization\n",
    "        viz_image = pipeline.visualize_shelf_detection(pil_image, result)\n",
    "        \n",
    "        # Convert to base64 for response\n",
    "        buffered = BytesIO()\n",
    "        viz_image.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Add visualization to result\n",
    "        result[\"visualization_base64\"] = img_str\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in shelf analysis: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Main function to run the API server\n",
    "def main():\n",
    "    logger.info(f\"Starting Retail GenAI API server on {HOST}:{PORT}\")\n",
    "    uvicorn.run(\"src.api.server:app\", host=HOST, port=PORT, reload=DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Create the API directory if it doesn't exist\n",
    "api_dir = REPO_ROOT / \"src\" / \"api\"\n",
    "os.makedirs(api_dir, exist_ok=True)\n",
    "\n",
    "# Write the server code to a file (commenting this out to avoid overwriting existing files)\n",
    "# with open(api_dir / \"server.py\", \"w\") as f:\n",
    "#     f.write(api_server_code)\n",
    "\n",
    "# Display the code instead\n",
    "print(api_server_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also create an inference pipeline module to support our API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example inference pipeline module (not to be executed directly)\n",
    "inference_pipeline_code = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Inference pipeline for the Retail GenAI system.\n",
    "\n",
    "This module implements an end-to-end inference pipeline that combines\n",
    "vision and language models for retail applications.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision.models as vision_models\n",
    "\n",
    "# Import project modules\n",
    "from src.models.multimodal_fusion import RetailProductFusionModel\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    \"\"\"Multi-modal classifier combining vision and language features.\"\"\"\n",
    "    \n",
    "    def __init__(self, fusion_model, num_classes):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_model = fusion_model\n",
    "        self.classifier = nn.Linear(fusion_model.output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img_features, text_features):\n",
    "        outputs = self.fusion_model(img_features=img_features, text_features=text_features)\n",
    "        embeddings = outputs[\"embeddings\"]\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "class RetailGenAIPipeline:\n",
    "    \"\"\"End-to-end inference pipeline for retail GenAI system.\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_model, language_model, tokenizer, classifier, \n",
    "                 transforms, category_mapping, device=\"cuda\"):\n",
    "        self.vision_model = vision_model\n",
    "        self.language_model = language_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classifier = classifier\n",
    "        self.transforms = transforms\n",
    "        self.category_mapping = category_mapping\n",
    "        self.device = device\n",
    "        \n",
    "        # Set all models to evaluation mode\n",
    "        self.vision_model.eval()\n",
    "        self.language_model.eval()\n",
    "        self.classifier.eval()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_dir, device=\"cuda\"):\n",
    "        \"\"\"Load a pipeline from pretrained models.\"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        \n",
    "        # Load model metadata\n",
    "        metadata_path = model_dir / \"multimodal_model_metadata.json\"\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                model_metadata = json.load(f)\n",
    "        else:\n",
    "            # Default metadata for demo purposes\n",
    "            model_metadata = {\n",
    "                \"model_type\": \"MultiModalClassifier\",\n",
    "                \"fusion_type\": \"attention\",\n",
    "                \"img_feature_dim\": 2048,  # For ResNet50\n",
    "                \"text_feature_dim\": 384,   # For MiniLM-L6\n",
    "                \"hidden_dim\": 512,\n",
    "                \"output_dim\": 256,\n",
    "                \"num_classes\": 5,  # Default to 5 common retail categories\n",
    "                \"category_mapping\": {\"0\": \"Electronics\", \"1\": \"Clothing\", \"2\": \"Groceries\", \"3\": \"Home\", \"4\": \"Beauty\"},\n",
    "                \"test_accuracy\": 0.85,\n",
    "                \"trained_on\": \"demo_dataset\",\n",
    "                \"date_trained\": \"2023-01-01\"\n",
    "            }\n",
    "        \n",
    "        # Fix category mapping keys (JSON converts all keys to strings)\n",
    "        category_mapping = {int(k): v for k, v in model_metadata[\"category_mapping\"].items()}\n",
    "        \n",
    "        # Load models\n",
    "        # 1. Vision model\n",
    "        vision_model = vision_models.resnet50(pretrained=True)\n",
    "        # Remove the classification layer\n",
    "        vision_model = nn.Sequential(*list(vision_model.children())[:-1])\n",
    "        vision_model = vision_model.to(device)\n",
    "        vision_model.eval()\n",
    "        \n",
    "        # 2. Language model\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        language_model = AutoModel.from_pretrained(model_name)\n",
    "        language_model = language_model.to(device)\n",
    "        language_model.eval()\n",
    "        \n",
    "        # 3. Fusion model\n",
    "        fusion_model = RetailProductFusionModel(\n",
    "            vision_encoder=None,  # We'll use pre-extracted features\n",
    "            text_encoder=None,    # We'll use pre-extracted features\n",
    "            fusion_type=model_metadata[\"fusion_type\"],\n",
    "            img_feature_dim=model_metadata[\"img_feature_dim\"],\n",
    "            text_feature_dim=model_metadata[\"text_feature_dim\"],\n",
    "            hidden_dim=model_metadata[\"hidden_dim\"],\n",
    "            output_dim=model_metadata[\"output_dim\"]\n",
    "        )\n",
    "        \n",
    "        # 4. Classifier\n",
    "        classifier = MultiModalClassifier(fusion_model, model_metadata[\"num_classes\"])\n",
    "        \n",
    "        # Load trained model if exists\n",
    "        model_path = model_dir / \"best_multimodal_classifier.pth\"\n",
    "        if model_path.exists():\n",
    "            classifier.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "        # Move model to device and set to evaluation mode\n",
    "        classifier = classifier.to(device)\n",
    "        classifier.eval()\n",
    "        \n",
    "        # Image transformations\n",
    "        image_transforms = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        return cls(\n",
    "            vision_model=vision_model,\n",
    "            language_model=language_model,\n",
    "            tokenizer=tokenizer,\n",
    "            classifier=classifier,\n",
    "            transforms=image_transforms,\n",
    "            category_mapping=category_mapping,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for model input.\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # Load from path\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        elif isinstance(image, bytes):\n",
    "            # Load from bytes\n",
    "            image = Image.open(BytesIO(image)).convert('RGB')\n",
    "        elif not isinstance(image, Image.Image):\n",
    "            raise ValueError(\"Image must be a PIL Image, a path string, or bytes\")\n",
    "            \n",
    "        # Apply transformations\n",
    "        return self.transforms(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    def preprocess_text(self, text, max_length=128):\n",
    "        \"\"\"Preprocess text for model input.\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "    def extract_features(self, image, text):\n",
    "        \"\"\"Extract features from vision and language models.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Process image\n",
    "            img_tensor = self.preprocess_image(image)\n",
    "            img_tensor = img_tensor.to(self.device)\n",
    "            img_features = self.vision_model(img_tensor).squeeze(-1).squeeze(-1)\n",
    "            \n",
    "            # Process text\n",
    "            text_encoding = self.preprocess_text(text)\n",
    "            input_ids = text_encoding[\"input_ids\"].to(self.device)\n",
    "            attention_mask = text_encoding[\"attention_mask\"].to(self.device)\n",
    "            text_outputs = self.language_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_features = text_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return img_features, text_features\n",
    "    \n",
    "    def predict(self, image, text):\n",
    "        \"\"\"Run end-to-end prediction pipeline.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract features\n",
    "        img_features, text_features = self.extract_features(image, text)\n",
    "        \n",
    "        # Run classifier\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(img_features, text_features)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)[0]\n",
    "            predicted_idx = torch.argmax(probs).item()\n",
    "            predicted_category = self.category_mapping[predicted_idx]\n",
    "            confidence = probs[predicted_idx].item()\n",
    "        \n",
    "        # Collect all probabilities\n",
    "        category_probs = {}\n",
    "        for idx, prob in enumerate(probs):\n",
    "            category = self.category_mapping[idx]\n",
    "            category_probs[category] = prob.item()\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"predicted_category\": predicted_category,\n",
    "            \"confidence\": confidence,\n",
    "            \"all_probabilities\": category_probs,\n",
    "            \"inference_time\": inference_time\n",
    "        }\n",
    "    \n",
    "    def process_shelf_image(self, image, text=\"\"):\n",
    "        \"\"\"Process a shelf image to identify products (using a simple detection model).\"\"\"\n",
    "        # In a production implementation, this would use a proper object detection model\n",
    "        # For this demo, we'll use a simplified approach\n",
    "        import random\n",
    "        \n",
    "        # If image is a path or bytes, convert to PIL Image\n",
    "        if isinstance(image, str):\n",
    "            img = Image.open(image).convert('RGB')\n",
    "        elif isinstance(image, bytes):\n",
    "            img = Image.open(BytesIO(image)).convert('RGB')\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Image must be a PIL Image, a path string, or bytes\")\n",
    "        \n",
    "        width, height = img.size\n",
    "        \n",
    "        # Simulated product detection for demo purposes\n",
    "        # In a real implementation, this would use a detection model like YOLO or Faster R-CNN\n",
    "        num_products = random.randint(3, 8)\n",
    "        product_detections = []\n",
    "        \n",
    "        for i in range(num_products):\n",
    "            # Generate random box (ensuring they don't go outside the image)\n",
    "            box_width = random.randint(width // 6, width // 3)\n",
    "            box_height = random.randint(height // 6, height // 3)\n",
    "            x = random.randint(0, width - box_width)\n",
    "            y = random.randint(0, height - box_height)\n",
    "            \n",
    "            # Random category and confidence\n",
    "            cat_idx = random.randint(0, len(self.category_mapping) - 1)\n",
    "            category = self.category_mapping[cat_idx]\n",
    "            confidence = random.uniform(0.7, 0.99)\n",
    "            \n",
    "            product_detections.append({\n",
    "                \"box\": [x, y, x + box_width, y + box_height],\n",
    "                \"category\": category,\n",
    "                \"confidence\": confidence,\n",
    "                \"product_id\": f\"P{random.randint(1000, 9999)}\"\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"num_products\": len(product_detections),\n",
    "            \"detections\": product_detections,\n",
    "            \"shelf_image_size\": [width, height]\n",
    "        }\n",
    "    \n",
    "    def visualize_shelf_detection(self, image, results):\n",
    "        \"\"\"Visualize product detections on a shelf image.\"\"\"\n",
    "        # Convert to PIL Image if needed\n",
    "        if not isinstance(image, Image.Image):\n",
    "            if isinstance(image, str):\n",
    "                image = Image.open(image).convert('RGB')\n",
    "            elif isinstance(image, bytes):\n",
    "                image = Image.open(BytesIO(image)).convert('RGB')\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                image = Image.fromarray(image)\n",
    "        \n",
    "        # Create a copy of the image to draw on\n",
    "        img_draw = image.copy()\n",
    "        draw = ImageDraw.Draw(img_draw)\n",
    "        \n",
    "        # Try to load a font (use default if not available)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Color mapping for categories\n",
    "        category_colors = {\n",
    "            \"Electronics\": \"blue\",\n",
    "            \"Clothing\": \"red\",\n",
    "            \"Groceries\": \"green\",\n",
    "            \"Home\": \"orange\",\n",
    "            \"Beauty\": \"purple\"\n",
    "        }\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        for det in results[\"detections\"]:\n",
    "            box = det[\"box\"]\n",
    "            category = det[\"category\"]\n",
    "            confidence = det[\"confidence\"]\n",
    "            product_id = det[\"product_id\"]\n",
    "            \n",
    "            # Get color for category\n",
    "            color = category_colors.get(category, \"gray\")\n",
    "            \n",
    "            # Draw rectangle\n",
    "            draw.rectangle(box, outline=color, width=3)\n",
    "            \n",
    "            # Draw label background\n",
    "            label = f\"{category} ({confidence:.1%})\"\n",
    "            label_size = draw.textbbox((0, 0), label, font=font)\n",
    "            label_width = label_size[2] - label_size[0]\n",
    "            label_height = label_size[3] - label_size[1]\n",
    "            label_bg = [box[0], box[1] - label_height - 4, box[0] + label_width + 4, box[1]]\n",
    "            draw.rectangle(label_bg, fill=color)\n",
    "            \n",
    "            # Draw label text\n",
    "            draw.text((box[0] + 2, box[1] - label_height - 2), label, fill=\"white\", font=font)\n",
    "        \n",
    "        return img_draw\n",
    "    \n",
    "    def answer_product_question(self, image, question):\n",
    "        \"\"\"Answer a natural language question about a product.\"\"\"\n",
    "        # First, predict the product category\n",
    "        prediction = self.predict(image, question)\n",
    "        category = prediction[\"predicted_category\"]\n",
    "        confidence = prediction[\"confidence\"]\n",
    "        \n",
    "        # Mock product catalog (in real implementation, this would query a database)\n",
    "        product_details = {\n",
    "            \"Electronics\": {\n",
    "                \"price_range\": \"$50-$1200\",\n",
    "                \"top_brands\": [\"TechCorp\", \"Electra\", \"DigiLife\"],\n",
    "                \"features\": [\"wireless connectivity\", \"long battery life\", \"high resolution display\"],\n",
    "                \"warranty\": \"1-3 years\"\n",
    "            },\n",
    "            \"Clothing\": {\n",
    "                \"price_range\": \"$15-$250\",\n",
    "                \"top_brands\": [\"StyleX\", \"UrbanFit\", \"ClassicWear\"],\n",
    "                \"features\": [\"sustainable materials\", \"comfortable fit\", \"machine washable\"],\n",
    "                \"warranty\": \"30-day returns\"\n",
    "            },\n",
    "            \"Groceries\": {\n",
    "                \"price_range\": \"$2-$35\",\n",
    "                \"top_brands\": [\"FreshFarms\", \"OrganicLife\", \"NatureHarvest\"],\n",
    "                \"features\": [\"organic options\", \"locally sourced\", \"no preservatives\"],\n",
    "                \"warranty\": \"satisfaction guarantee\"\n",
    "            },\n",
    "            \"Home\": {\n",
    "                \"price_range\": \"$10-$500\",\n",
    "                \"top_brands\": [\"HomeLux\", \"ComfortLiving\", \"ModernSpace\"],\n",
    "                \"features\": [\"durable construction\", \"stylish design\", \"easy assembly\"],\n",
    "                \"warranty\": \"1-5 years\"\n",
    "            },\n",
    "            \"Beauty\": {\n",
    "                \"price_range\": \"$8-$150\",\n",
    "                \"top_brands\": [\"GlowUp\", \"NaturalBeauty\", \"LuxeSkin\"],\n",
    "                \"features\": [\"cruelty-free\", \"fragrance-free options\", \"dermatologist tested\"],\n",
    "                \"warranty\": \"30-day returns\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Simple rule-based QA logic\n",
    "        response = \"\"\n",
    "        \n",
    "        # Get product info for the predicted category\n",
    "        if category in product_details:\n",
    "            info = product_details[category]\n",
    "            \n",
    "            # Very basic keyword matching for demo purposes\n",
    "            question_lower = question.lower()\n",
    "            \n",
    "            if \"price\" in question_lower or \"cost\" in question_lower or \"how much\" in question_lower:\n",
    "                response = f\"This {category} product typically costs in the range of {info['price_range']}.\"\n",
    "            \n",
    "            elif \"brand\" in question_lower or \"who makes\" in question_lower or \"manufacturer\" in question_lower:\n",
    "                top_brands = \", \".join(info[\"top_brands\"])\n",
    "                response = f\"The top brands in this {category} category include {top_brands}.\"\n",
    "            \n",
    "            elif \"feature\" in question_lower or \"specification\" in question_lower or \"what can\" in question_lower:\n",
    "                features = \", \".join(info[\"features\"])\n",
    "                response = f\"This {category} product typically offers these features: {features}.\"\n",
    "            \n",
    "            elif \"warranty\" in question_lower or \"guarantee\" in question_lower or \"return\" in question_lower:\n",
    "                response = f\"This {category} product typically comes with a {info['warranty']}.\"\n",
    "            \n",
    "            elif \"recommend\" in question_lower or \"alternative\" in question_lower or \"similar\" in question_lower:\n",
    "                response = f\"Based on this {category} product, I would recommend checking out items from {', '.join(info['top_brands'][:2])}.\"\n",
    "            \n",
    "            else:\n",
    "                # Generic response for other questions\n",
    "                response = f\"This appears to be a {category} product. It typically costs {info['price_range']} and features {', '.join(info['features'][:2])}.\"\n",
    "        else:\n",
    "            response = \"I couldn't identify the product category clearly. Could you provide more information?\"\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"predicted_category\": category,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "# Create the inference directory if it doesn't exist\n",
    "inference_dir = REPO_ROOT / \"src\" / \"inference\"\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "\n",
    "# Write the pipeline code to a file (commenting this out to avoid overwriting existing files)\n",
    "# with open(inference_dir / \"pipeline.py\", \"w\") as f:\n",
    "#     f.write(inference_pipeline_code)\n",
    "\n",
    "# Display the code instead\n",
    "print(inference_pipeline_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Kubernetes Deployment for Production\n",
    "\n",
    "For production deployment, Kubernetes provides better scalability and management. Here's an example of Kubernetes manifests for our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example Kubernetes deployment manifest\n",
    "kubernetes_deployment = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: retail-genai\n",
    "  labels:\n",
    "    app: retail-genai\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: retail-genai\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: retail-genai\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: retail-genai\n",
    "        image: retail-genai-accelerator:latest\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        command: [\"python\", \"-m\", \"src.api.server\"]\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"1\"\n",
    "        env:\n",
    "        - name: MODEL_DIR\n",
    "          value: \"/app/models\"\n",
    "        - name: USE_GPU\n",
    "          value: \"true\"\n",
    "        - name: HOST\n",
    "          value: \"0.0.0.0\"\n",
    "        - name: PORT\n",
    "          value: \"8000\"\n",
    "        - name: DEBUG\n",
    "          value: \"false\"\n",
    "        volumeMounts:\n",
    "        - name: models-volume\n",
    "          mountPath: /app/models\n",
    "        - name: data-volume\n",
    "          mountPath: /app/data\n",
    "      volumes:\n",
    "      - name: models-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: models-pvc\n",
    "      - name: data-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: data-pvc\n",
    "\"\"\"\n",
    "\n",
    "# Example Kubernetes service manifest\n",
    "kubernetes_service = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: retail-genai\n",
    "spec:\n",
    "  selector:\n",
    "    app: retail-genai\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "  type: ClusterIP\n",
    "\"\"\"\n",
    "\n",
    "# Example Kubernetes ingress manifest\n",
    "kubernetes_ingress = \"\"\"\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: retail-genai-ingress\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "spec:\n",
    "  rules:\n",
    "  - host: retail-genai.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: retail-genai\n",
    "            port:\n",
    "              number: 8000\n",
    "\"\"\"\n",
    "\n",
    "# Example Kubernetes persistent volume claims\n",
    "kubernetes_pvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: models-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: data-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 20Gi\n",
    "\"\"\"\n",
    "\n",
    "# Create k8s directory\n",
    "k8s_dir = REPO_ROOT / \"k8s\"\n",
    "os.makedirs(k8s_dir, exist_ok=True)\n",
    "\n",
    "# Write Kubernetes manifests (commenting this out to avoid overwriting existing files)\n",
    "# with open(k8s_dir / \"deployment.yaml\", \"w\") as f:\n",
    "#     f.write(kubernetes_deployment)\n",
    "# with open(k8s_dir / \"service.yaml\", \"w\") as f:\n",
    "#     f.write(kubernetes_service)\n",
    "# with open(k8s_dir / \"ingress.yaml\", \"w\") as f:\n",
    "#     f.write(kubernetes_ingress)\n",
    "# with open(k8s_dir / \"pvc.yaml\", \"w\") as f:\n",
    "#     f.write(kubernetes_pvc)\n",
    "\n",
    "# Display the Kubernetes manifests\n",
    "print(\"Kubernetes Deployment Manifest:\\n\")\n",
    "print(kubernetes_deployment)\n",
    "print(\"\\nKubernetes Service Manifest:\\n\")\n",
    "print(kubernetes_service)\n",
    "print(\"\\nKubernetes Ingress Manifest:\\n\")\n",
    "print(kubernetes_ingress)\n",
    "print(\"\\nKubernetes PVC Manifest:\\n\")\n",
    "print(kubernetes_pvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deploying to Kubernetes with NVIDIA GPU Support\n",
    "\n",
    "Here are the steps to deploy our application to a Kubernetes cluster with NVIDIA GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Commands to deploy to Kubernetes (not executed)\n",
    "kubernetes_deploy_commands = \"\"\"\n",
    "# 1. Build and push Docker image (if using a registry)\n",
    "docker build -t retail-genai-accelerator:latest -f docker/Dockerfile .\n",
    "docker tag retail-genai-accelerator:latest your-registry/retail-genai-accelerator:latest\n",
    "docker push your-registry/retail-genai-accelerator:latest\n",
    "\n",
    "# 2. Apply Kubernetes manifests\n",
    "kubectl apply -f k8s/pvc.yaml\n",
    "kubectl apply -f k8s/deployment.yaml\n",
    "kubectl apply -f k8s/service.yaml\n",
    "kubectl apply -f k8s/ingress.yaml\n",
    "\n",
    "# 3. Verify deployment\n",
    "kubectl get pods\n",
    "kubectl get services\n",
    "kubectl get ingress\n",
    "\n",
    "# 4. Check logs\n",
    "kubectl logs -f deployment/retail-genai\n",
    "\n",
    "# 5. Check GPU utilization within the pod\n",
    "kubectl exec -it $(kubectl get pods -l app=retail-genai -o jsonpath='{.items[0].metadata.name}') -- nvidia-smi\n",
    "\"\"\"\n",
    "\n",
    "print(\"Kubernetes Deployment Commands:\")\n",
    "print(kubernetes_deploy_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA GPU Operator for Kubernetes\n",
    "\n",
    "The NVIDIA GPU Operator simplifies the management of GPUs in Kubernetes clusters. It automatically installs drivers, device plugins, and monitoring components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# NVIDIA GPU Operator installation (not executed)\n",
    "gpu_operator_install = \"\"\"\n",
    "# Add the NVIDIA Helm repository\n",
    "helm repo add nvidia https://nvidia.github.io/gpu-operator\n",
    "helm repo update\n",
    "\n",
    "# Install the NVIDIA GPU Operator\n",
    "helm install --wait --generate-name \\\n",
    "     -n gpu-operator --create-namespace \\\n",
    "     nvidia/gpu-operator\n",
    "\n",
    "# Verify installation\n",
    "kubectl get pods -n gpu-operator\n",
    "\"\"\"\n",
    "\n",
    "print(\"NVIDIA GPU Operator Installation:\")\n",
    "print(gpu_operator_install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Monitoring\n",
    "\n",
    "For monitoring GPU utilization and application performance in production, we can use tools like Prometheus, Grafana, and NVIDIA DCGM (Data Center GPU Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prometheus configuration for GPU monitoring (not executed)\n",
    "prometheus_config = \"\"\"\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'kubernetes-pods'\n",
    "    kubernetes_sd_configs:\n",
    "    - role: pod\n",
    "    relabel_configs:\n",
    "    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n",
    "      action: keep\n",
    "      regex: true\n",
    "    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n",
    "      action: replace\n",
    "      target_label: __metrics_path__\n",
    "      regex: (.+)\n",
    "    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n",
    "      action: replace\n",
    "      regex: ([^:]+)(?::\\d+)?;(\\d+)\n",
    "      replacement: $1:$2\n",
    "      target_label: __address__\n",
    "\n",
    "  - job_name: 'dcgm-exporter'\n",
    "    kubernetes_sd_configs:\n",
    "    - role: pod\n",
    "    relabel_configs:\n",
    "    - source_labels: [__meta_kubernetes_pod_label_app]\n",
    "      action: keep\n",
    "      regex: dcgm-exporter\n",
    "    - source_labels: [__address__]\n",
    "      action: replace\n",
    "      regex: ([^:]+)(?::\\d+)?\n",
    "      replacement: $1:9400\n",
    "      target_label: __address__\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prometheus Configuration for GPU Monitoring:\")\n",
    "print(prometheus_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafana Dashboard for GPU Monitoring\n",
    "\n",
    "Here's an example of how to set up a Grafana dashboard for GPU monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# NVIDIA DCGM installation with Helm (not executed)\n",
    "dcgm_install = \"\"\"\n",
    "# Add the NVIDIA Helm repository (if not already added)\n",
    "helm repo add nvidia https://nvidia.github.io/gpu-monitoring-tools/helm-charts\n",
    "helm repo update\n",
    "\n",
    "# Install DCGM Exporter\n",
    "helm install --name=dcgm-exporter nvidia/dcgm-exporter\n",
    "\n",
    "# View the metrics\n",
    "kubectl port-forward svc/dcgm-exporter 9400:9400\n",
    "# Then visit http://localhost:9400/metrics in your browser\n",
    "\"\"\"\n",
    "\n",
    "print(\"NVIDIA DCGM Exporter Installation:\")\n",
    "print(dcgm_install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scaling Strategies\n",
    "\n",
    "Here are some strategies for scaling our application based on load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Horizontal Pod Autoscaler example (not executed)\n",
    "hpa_manifest = \"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: retail-genai-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: retail-genai\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "\"\"\"\n",
    "\n",
    "print(\"Horizontal Pod Autoscaler Manifest:\")\n",
    "print(hpa_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Node GPU Cluster\n",
    "\n",
    "For handling larger workloads, we can deploy our application across multiple nodes with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Multi-node deployment strategies (not executed)\n",
    "multi_node_strategies = \"\"\"\n",
    "# 1. Node Selection\n",
    "# Use nodeSelector or node affinity to target specific GPU nodes\n",
    "nodeSelector:\n",
    "  gpu-type: nvidia-a100\n",
    "\n",
    "# 2. Anti-Affinity\n",
    "# Spread pods across nodes for high availability\n",
    "affinity:\n",
    "  podAntiAffinity:\n",
    "    preferredDuringSchedulingIgnoredDuringExecution:\n",
    "    - weight: 100\n",
    "      podAffinityTerm:\n",
    "        labelSelector:\n",
    "          matchExpressions:\n",
    "          - key: app\n",
    "            operator: In\n",
    "            values:\n",
    "            - retail-genai\n",
    "        topologyKey: kubernetes.io/hostname\n",
    "\n",
    "# 3. Using Topology Spread Constraints\n",
    "# More advanced scheduling control\n",
    "topologySpreadConstraints:\n",
    "- maxSkew: 1\n",
    "  topologyKey: kubernetes.io/hostname\n",
    "  whenUnsatisfiable: DoNotSchedule\n",
    "  labelSelector:\n",
    "    matchLabels:\n",
    "      app: retail-genai\n",
    "\"\"\"\n",
    "\n",
    "print(\"Multi-Node Deployment Strategies:\")\n",
    "print(multi_node_strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Security Considerations\n",
    "\n",
    "Here are some security considerations for deploying containerized ML applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Security best practices (not executed)\n",
    "security_best_practices = \"\"\"\n",
    "# 1. Use Non-Root User in Containers\n",
    "# Add to Dockerfile:\n",
    "RUN groupadd -r appuser && useradd -r -g appuser appuser\n",
    "USER appuser\n",
    "\n",
    "# 2. Set Security Context in Kubernetes\n",
    "securityContext:\n",
    "  runAsUser: 1000\n",
    "  runAsGroup: 1000\n",
    "  runAsNonRoot: true\n",
    "  readOnlyRootFilesystem: true\n",
    "\n",
    "# 3. Resource Limits\n",
    "resources:\n",
    "  limits:\n",
    "    nvidia.com/gpu: 1\n",
    "    memory: \"8Gi\"\n",
    "    cpu: \"2\"\n",
    "  requests:\n",
    "    memory: \"4Gi\"\n",
    "    cpu: \"1\"\n",
    "\n",
    "# 4. Network Policies\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: retail-genai-network-policy\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: retail-genai\n",
    "  ingress:\n",
    "  - from:\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app: frontend\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "\n",
    "# 5. Secrets Management\n",
    "# Use Kubernetes Secrets for credentials\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: retail-genai-secrets\n",
    "type: Opaque\n",
    "data:\n",
    "  api-key: base64EncodedApiKey\n",
    "\"\"\"\n",
    "\n",
    "print(\"Security Best Practices:\")\n",
    "print(security_best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. CI/CD Pipeline for ML Models\n",
    "\n",
    "Setting up a CI/CD pipeline for ML models ensures reliable, reproducible deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# GitHub Actions CI/CD example (not executed)\n",
    "github_actions_workflow = \"\"\"\n",
    "name: Retail GenAI CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.10'\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install pytest pytest-cov\n",
    "        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "    - name: Test with pytest\n",
    "      run: |\n",
    "        pytest --cov=src tests/\n",
    "\n",
    "  build-and-push:\n",
    "    needs: test\n",
    "    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Login to DockerHub\n",
    "      uses: docker/login-action@v1\n",
    "      with:\n",
    "        username: ${{ secrets.DOCKERHUB_USERNAME }}\n",
    "        password: ${{ secrets.DOCKERHUB_TOKEN }}\n",
    "    - name: Build and push\n",
    "      uses: docker/build-push-action@v2\n",
    "      with:\n",
    "        context: .\n",
    "        file: ./docker/Dockerfile\n",
    "        push: true\n",
    "        tags: yourusername/retail-genai-accelerator:latest\n",
    "\n",
    "  deploy:\n",
    "    needs: build-and-push\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up kubectl\n",
    "      uses: azure/setup-kubectl@v1\n",
    "    - name: Set up kubeconfig\n",
    "      run: |\n",
    "        mkdir -p $HOME/.kube\n",
    "        echo \"${{ secrets.KUBE_CONFIG }}\" > $HOME/.kube/config\n",
    "    - name: Deploy to Kubernetes\n",
    "      run: |\n",
    "        kubectl apply -f k8s/pvc.yaml\n",
    "        kubectl apply -f k8s/deployment.yaml\n",
    "        kubectl apply -f k8s/service.yaml\n",
    "        kubectl apply -f k8s/ingress.yaml\n",
    "        kubectl rollout restart deployment/retail-genai\n",
    "\"\"\"\n",
    "\n",
    "print(\"GitHub Actions CI/CD Workflow:\")\n",
    "print(github_actions_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've covered the containerization and deployment of our retail GenAI system. Key points include:\n",
    "\n",
    "1. **Docker Containerization**: We've packaged our application in a Docker container with NVIDIA GPU support.\n",
    "\n",
    "2. **Kubernetes Deployment**: We've prepared Kubernetes manifests for production deployment with GPU acceleration.\n",
    "\n",
    "3. **API Server**: We've created a dedicated API server for serving model predictions.\n",
    "\n",
    "4. **Infrastructure as Code**: We've provided all the necessary configuration files for reproducible deployments.\n",
    "\n",
    "5. **Performance Monitoring**: We've added monitoring tools for tracking GPU utilization and application performance.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your retail GenAI system, consider the following next steps:\n",
    "\n",
    "1. **Implement Active Learning**: Add feedback loops to continuously improve model performance.\n",
    "\n",
    "2. **Model Versioning**: Set up model versioning and A/B testing infrastructure.\n",
    "\n",
    "3. **Advanced GPU Optimizations**: Explore NVIDIA Triton Inference Server for even better performance.\n",
    "\n",
    "4. **Edge Deployment**: Adapt the system for deployment on edge devices in retail stores.\n",
    "\n",
    "5. **Integration with Retail Systems**: Connect with inventory management, CRM, and POS systems.\n",
    "\n",
    "This comprehensive solution demonstrates the power of combining AI with NVIDIA GPU acceleration for retail applications, providing a foundation for real-world deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
