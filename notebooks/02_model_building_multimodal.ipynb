{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Model Building for Retail GenAI System\n",
    "\n",
    "This notebook demonstrates how to build multi-modal models that combine:\n",
    "1. Vision models (for product recognition)\n",
    "2. Language models (for text understanding)\n",
    "3. Fusion techniques (for combining modalities)\n",
    "\n",
    "We'll leverage NVIDIA GPUs to accelerate both training and inference, showing the performance benefits of GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our GPU-accelerated environment and load necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing project modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import project-specific modules\n",
    "from src.models.multimodal_fusion import RetailProductFusionModel, create_nvidia_optimized_fusion_model\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Models\n",
    "\n",
    "We'll start by loading pre-trained vision and language models that will form the foundation of our multi-modal system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install -q transformers pillow opencv-python torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
    "import torchvision.models as vision_models\n",
    "\n",
    "# Function to measure model loading time\n",
    "def time_model_loading(func):\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    end = time.time()\n",
    "    print(f\"Loading time: {end - start:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "# Load vision model\n",
    "print(\"Loading vision model...\")\n",
    "@time_model_loading\n",
    "def load_vision_model():\n",
    "    # Use a pre-trained ResNet model\n",
    "    model = vision_models.resnet50(pretrained=True)\n",
    "    # Remove the classification layer\n",
    "    features = nn.Sequential(*list(model.children())[:-1])\n",
    "    features.to(device)\n",
    "    features.eval()\n",
    "    return features\n",
    "\n",
    "vision_model = load_vision_model()\n",
    "\n",
    "# Load language model\n",
    "print(\"\\nLoading language model...\")\n",
    "@time_model_loading\n",
    "def load_language_model():\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, language_model = load_language_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Retail Dataset\n",
    "\n",
    "Let's load the processed retail dataset that we prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths\n",
    "REPO_ROOT = Path(\"..\")\n",
    "PROCESSED_DATA_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "RAW_DATA_DIR = REPO_ROOT / \"examples\" / \"product_data\"\n",
    "IMAGE_DIR = REPO_ROOT / \"examples\" / \"images\"\n",
    "\n",
    "# Check if processed data exists, otherwise use raw data\n",
    "if PROCESSED_DATA_DIR.exists() and (PROCESSED_DATA_DIR / \"processed_product_catalog.csv\").exists():\n",
    "    print(\"Loading processed data...\")\n",
    "    products_df = pd.read_csv(PROCESSED_DATA_DIR / \"processed_product_catalog.csv\")\n",
    "else:\n",
    "    print(\"Processed data not found. Loading raw data...\")\n",
    "    # Check if raw data exists\n",
    "    if RAW_DATA_DIR.exists() and any(RAW_DATA_DIR.glob(\"*.csv\")):\n",
    "        catalog_file = next(RAW_DATA_DIR.glob(\"*.csv\"))\n",
    "        products_df = pd.read_csv(catalog_file)\n",
    "    else:\n",
    "        print(\"No data found. Running data generation script...\")\n",
    "        # Import and run the data generation script\n",
    "        import src.utils.download_demo_data as data_gen\n",
    "        os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "        products_df, _, _ = data_gen.generate_sample_data(RAW_DATA_DIR)\n",
    "        \n",
    "    # Add full_text field if not present\n",
    "    if 'full_text' not in products_df.columns:\n",
    "        products_df['full_text'] = (\n",
    "            'Product: ' + products_df['name'].astype(str) + '. ' +\n",
    "            'Category: ' + products_df['category'].astype(str) + '. ' +\n",
    "            'Price: $' + products_df['price'].astype(str) + '. ' +\n",
    "            'Description: ' + products_df['description'].astype(str) + '. ' +\n",
    "            'In stock: ' + products_df['in_stock'].map({True: 'Yes', False: 'No'}).astype(str)\n",
    "        )\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset loaded with {len(products_df)} products\")\n",
    "print(\"\\nSample product:\")\n",
    "display(products_df.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Multi-Modal Dataset\n",
    "\n",
    "We need to create a dataset that combines product images and text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# Define image transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create a placeholder image generator for demo purposes\n",
    "def generate_placeholder_image(product_id, category):\n",
    "    \"\"\"Generate a placeholder image based on product info.\"\"\"\n",
    "    # Create a colored background based on category\n",
    "    category_colors = {\n",
    "        \"Electronics\": (200, 200, 255),  # Light blue\n",
    "        \"Clothing\": (255, 200, 200),     # Light red\n",
    "        \"Groceries\": (200, 255, 200),    # Light green\n",
    "        \"Home\": (255, 255, 200),         # Light yellow\n",
    "        \"Beauty\": (255, 200, 255),       # Light purple\n",
    "        \"default\": (240, 240, 240)       # Light gray\n",
    "    }\n",
    "    \n",
    "    color = category_colors.get(category, category_colors[\"default\"])\n",
    "    img = np.ones((224, 224, 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
    "    \n",
    "    # Add a product ID text\n",
    "    cv2.putText(img, f\"Product {product_id}\", (30, 112), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb)\n",
    "\n",
    "# Define a PyTorch dataset\n",
    "class RetailMultiModalDataset(data.Dataset):\n",
    "    def __init__(self, products_df, image_dir=None, transform=None, tokenizer=None, max_length=128):\n",
    "        self.products_df = products_df\n",
    "        self.image_dir = Path(image_dir) if image_dir else None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.products_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get product data\n",
    "        product = self.products_df.iloc[idx]\n",
    "        product_id = product['product_id']\n",
    "        text = product['full_text']\n",
    "        category = product['category']\n",
    "        \n",
    "        # Get image (or generate a placeholder)\n",
    "        if self.image_dir:\n",
    "            img_path = self.image_dir / f\"product_{product_id}.jpg\"\n",
    "            if img_path.exists():\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            else:\n",
    "                # Generate a placeholder image if real image doesn't exist\n",
    "                image = generate_placeholder_image(product_id, category)\n",
    "        else:\n",
    "            image = generate_placeholder_image(product_id, category)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Tokenize text\n",
    "        if self.tokenizer:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"].squeeze()\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        else:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"product_id\": product_id,\n",
    "            \"category\": category,\n",
    "            \"text\": text\n",
    "        }\n",
    "\n",
    "# Create dataset and split into train/val/test\n",
    "def create_data_splits(products_df, val_ratio=0.15, test_ratio=0.15):\n",
    "    # Create a stratified split based on product categories\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First, split into train+val and test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        products_df, \n",
    "        test_size=test_ratio,\n",
    "        stratify=products_df['category'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Then split train+val into train and val\n",
    "    val_ratio_adjusted = val_ratio / (1 - test_ratio)  # Adjust for previous split\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_ratio_adjusted,\n",
    "        stratify=train_val_df['category'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} products\")\n",
    "    print(f\"Validation set: {len(val_df)} products\")\n",
    "    print(f\"Test set: {len(test_df)} products\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df, test_df = create_data_splits(products_df)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RetailMultiModalDataset(\n",
    "    train_df, \n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=image_transforms,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "val_dataset = RetailMultiModalDataset(\n",
    "    val_df, \n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=image_transforms,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = RetailMultiModalDataset(\n",
    "    test_df, \n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=image_transforms,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4 if torch.cuda.is_available() else 0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4 if torch.cuda.is_available() else 0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4 if torch.cuda.is_available() else 0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few samples from our dataset to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def show_batch(dataloader, num_samples=4):\n",
    "    # Get a batch\n",
    "    for batch in dataloader:\n",
    "        break\n",
    "    \n",
    "    # Display images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(16, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Convert tensor to image\n",
    "        img = batch['image'][i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Product {batch['product_id'][i].item()}\\n{batch['category'][i]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display text samples\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Text for product {batch['product_id'][i].item()}: {batch['text'][i][:100]}...\")\n",
    "\n",
    "# Show a batch from training data\n",
    "print(\"Sample from training set:\")\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Multi-Modal Fusion Model\n",
    "\n",
    "Now let's build our multi-modal fusion model that combines the visual and textual features. We'll leverage the `RetailProductFusionModel` class that we've already implemented in our codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure model parameters\n",
    "MODEL_CONFIG = {\n",
    "    \"img_feature_dim\": 2048,  # For ResNet50\n",
    "    \"text_feature_dim\": 384,   # For MiniLM-L6\n",
    "    \"hidden_dim\": 512,\n",
    "    \"output_dim\": 256,\n",
    "    \"fusion_type\": \"attention\",  # Options: \"concat\", \"attention\", \"gated\"\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# Create the multi-modal fusion model\n",
    "fusion_model = RetailProductFusionModel(\n",
    "    vision_encoder=None,  # We'll use pre-extracted features\n",
    "    text_encoder=None,    # We'll use pre-extracted features\n",
    "    fusion_type=MODEL_CONFIG[\"fusion_type\"],\n",
    "    img_feature_dim=MODEL_CONFIG[\"img_feature_dim\"],\n",
    "    text_feature_dim=MODEL_CONFIG[\"text_feature_dim\"],\n",
    "    hidden_dim=MODEL_CONFIG[\"hidden_dim\"],\n",
    "    output_dim=MODEL_CONFIG[\"output_dim\"],\n",
    "    dropout=MODEL_CONFIG[\"dropout\"]\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "fusion_model = fusion_model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Multi-Modal Fusion Model:\")\n",
    "print(fusion_model)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTrainable parameters: {count_parameters(fusion_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Before we train the fusion model, let's pre-extract features from our pre-trained vision and language models. This is an important step for efficiency, especially when using large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_features(dataloader, vision_model, language_model):\n",
    "    \"\"\"Extract features from both vision and language models.\"\"\"\n",
    "    vision_model.eval()\n",
    "    language_model.eval()\n",
    "    \n",
    "    all_img_features = []\n",
    "    all_text_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Move inputs to device\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"category\"]\n",
    "            \n",
    "            # Extract image features\n",
    "            img_features = vision_model(images)\n",
    "            img_features = img_features.view(img_features.size(0), -1)  # Flatten\n",
    "            \n",
    "            # Extract text features\n",
    "            text_outputs = language_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "            \n",
    "            # Store features and labels\n",
    "            all_img_features.append(img_features.cpu())\n",
    "            all_text_features.append(text_features.cpu())\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # Concatenate features\n",
    "    all_img_features = torch.cat(all_img_features, dim=0)\n",
    "    all_text_features = torch.cat(all_text_features, dim=0)\n",
    "    \n",
    "    return all_img_features, all_text_features, all_labels\n",
    "\n",
    "# Extract features for all datasets\n",
    "print(\"Extracting features for training set...\")\n",
    "train_img_features, train_text_features, train_labels = extract_features(train_loader, vision_model, language_model)\n",
    "\n",
    "print(\"\\nExtracting features for validation set...\")\n",
    "val_img_features, val_text_features, val_labels = extract_features(val_loader, vision_model, language_model)\n",
    "\n",
    "print(\"\\nExtracting features for test set...\")\n",
    "test_img_features, test_text_features, test_labels = extract_features(test_loader, vision_model, language_model)\n",
    "\n",
    "# Print feature dimensions\n",
    "print(f\"\\nImage features shape: {train_img_features.shape}\")\n",
    "print(f\"Text features shape: {train_text_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Fusion Model\n",
    "\n",
    "Now we'll train the fusion model using our extracted features. We'll use a supervised task of product category prediction to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# First, let's create label mappings for our categories\n",
    "unique_categories = products_df['category'].unique()\n",
    "category_to_idx = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "idx_to_category = {i: cat for cat, i in category_to_idx.items()}\n",
    "\n",
    "print(f\"Category mappings: {category_to_idx}\")\n",
    "\n",
    "# Create numeric labels\n",
    "train_numeric_labels = torch.tensor([category_to_idx[label] for label in train_labels])\n",
    "val_numeric_labels = torch.tensor([category_to_idx[label] for label in val_labels])\n",
    "test_numeric_labels = torch.tensor([category_to_idx[label] for label in test_labels])\n",
    "\n",
    "# Create a feature dataset\n",
    "class FeatureDataset(data.Dataset):\n",
    "    def __init__(self, img_features, text_features, labels):\n",
    "        self.img_features = img_features\n",
    "        self.text_features = text_features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"img_features\": self.img_features[idx],\n",
    "            \"text_features\": self.text_features[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create feature datasets and loaders\n",
    "train_feature_dataset = FeatureDataset(train_img_features, train_text_features, train_numeric_labels)\n",
    "val_feature_dataset = FeatureDataset(val_img_features, val_text_features, val_numeric_labels)\n",
    "test_feature_dataset = FeatureDataset(test_img_features, test_text_features, test_numeric_labels)\n",
    "\n",
    "train_feature_loader = DataLoader(train_feature_dataset, batch_size=64, shuffle=True)\n",
    "val_feature_loader = DataLoader(val_feature_dataset, batch_size=128, shuffle=False)\n",
    "test_feature_loader = DataLoader(test_feature_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a classification model that uses our fusion model\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, fusion_model, num_classes):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_model = fusion_model\n",
    "        self.classifier = nn.Linear(fusion_model.output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img_features, text_features):\n",
    "        # Get fusion model outputs\n",
    "        outputs = self.fusion_model(img_features=img_features, text_features=text_features)\n",
    "        embeddings = outputs[\"embeddings\"]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "# Create the classifier\n",
    "num_classes = len(unique_categories)\n",
    "classifier = MultiModalClassifier(fusion_model, num_classes).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move inputs to device\n",
    "        img_features = batch[\"img_features\"].to(device)\n",
    "        text_features = batch[\"text_features\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(img_features, text_features)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move inputs to device\n",
    "            img_features = batch[\"img_features\"].to(device)\n",
    "            text_features = batch[\"text_features\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(img_features, text_features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels for detailed metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(REPO_ROOT / \"models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(classifier, train_feature_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = evaluate(classifier, val_feature_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(classifier.state_dict(), REPO_ROOT / \"models\" / \"best_multimodal_classifier.pth\")\n",
    "        print(\"Saved best model checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curves')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Analysis\n",
    "\n",
    "Now let's evaluate our model on the test set and analyze its performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model\n",
    "best_model_path = REPO_ROOT / \"models\" / \"best_multimodal_classifier.pth\"\n",
    "classifier.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(classifier, test_feature_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Detailed classification metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert numeric labels back to category names for readability\n",
    "test_pred_categories = [idx_to_category[pred] for pred in test_preds]\n",
    "test_true_categories = [idx_to_category[label] for label in test_labels]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_true_categories, test_pred_categories))\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=list(idx_to_category.values()),\n",
    "            yticklabels=list(idx_to_category.values()))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis: Multi-Modal vs. Single-Modal\n",
    "\n",
    "Let's compare the performance of our multi-modal model with single-modal baselines to demonstrate the benefits of multi-modal fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create single-modal classifiers\n",
    "class ImageOnlyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_classes):\n",
    "        super(ImageOnlyClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        self.classifier = nn.Linear(output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img_features):\n",
    "        features = self.network(img_features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "class TextOnlyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_classes):\n",
    "        super(TextOnlyClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        self.classifier = nn.Linear(output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, text_features):\n",
    "        features = self.network(text_features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Initialize models\n",
    "img_classifier = ImageOnlyClassifier(\n",
    "    input_dim=MODEL_CONFIG[\"img_feature_dim\"],\n",
    "    hidden_dim=MODEL_CONFIG[\"hidden_dim\"],\n",
    "    output_dim=MODEL_CONFIG[\"output_dim\"],\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "text_classifier = TextOnlyClassifier(\n",
    "    input_dim=MODEL_CONFIG[\"text_feature_dim\"],\n",
    "    hidden_dim=MODEL_CONFIG[\"hidden_dim\"],\n",
    "    output_dim=MODEL_CONFIG[\"output_dim\"],\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Define optimizers\n",
    "img_optimizer = torch.optim.AdamW(img_classifier.parameters(), lr=1e-4)\n",
    "text_optimizer = torch.optim.AdamW(text_classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# Simplified training functions for single-modal models\n",
    "def train_image_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Image Model (Epoch {epoch+1}/{epochs})\"):\n",
    "            img_features = batch[\"img_features\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(img_features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating Image Model\"):\n",
    "                img_features = batch[\"img_features\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                \n",
    "                logits = model(img_features)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = running_loss / total\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), REPO_ROOT / \"models\" / \"best_image_classifier.pth\")\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "def train_text_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Text Model (Epoch {epoch+1}/{epochs})\"):\n",
    "            text_features = batch[\"text_features\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(text_features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating Text Model\"):\n",
    "                text_features = batch[\"text_features\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                \n",
    "                logits = model(text_features)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = running_loss / total\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), REPO_ROOT / \"models\" / \"best_text_classifier.pth\")\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "# Train single-modal models\n",
    "print(\"Training Image-Only Model...\")\n",
    "img_best_val_acc = train_image_model(img_classifier, train_feature_loader, val_feature_loader, img_optimizer, criterion)\n",
    "\n",
    "print(\"\\nTraining Text-Only Model...\")\n",
    "text_best_val_acc = train_text_model(text_classifier, train_feature_loader, val_feature_loader, text_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate all models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load best single-modal models\n",
    "img_classifier.load_state_dict(torch.load(REPO_ROOT / \"models\" / \"best_image_classifier.pth\"))\n",
    "text_classifier.load_state_dict(torch.load(REPO_ROOT / \"models\" / \"best_text_classifier.pth\"))\n",
    "\n",
    "# Evaluate image model\n",
    "img_classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "img_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_feature_loader, desc=\"Evaluating Image Model\"):\n",
    "        img_features = batch[\"img_features\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        logits = img_classifier(img_features)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        img_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "img_test_acc = correct / total\n",
    "print(f\"Image-Only Model Test Accuracy: {img_test_acc:.4f}\")\n",
    "\n",
    "# Evaluate text model\n",
    "text_classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "text_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_feature_loader, desc=\"Evaluating Text Model\"):\n",
    "        text_features = batch[\"text_features\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        logits = text_classifier(text_features)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        text_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "text_test_acc = correct / total\n",
    "print(f\"Text-Only Model Test Accuracy: {text_test_acc:.4f}\")\n",
    "\n",
    "# Reminder of multi-modal model accuracy\n",
    "print(f\"Multi-Modal Model Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the models in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare the performance of all models\n",
    "model_names = ['Image-Only', 'Text-Only', 'Multi-Modal']\n",
    "accuracies = [img_test_acc, text_test_acc, test_acc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'coral'])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title('Model Comparison: Test Accuracy', fontsize=14)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GPU Performance Analysis\n",
    "\n",
    "Let's analyze the performance benefits of using NVIDIA GPUs for our multi-modal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_inference(model, dataloader, device, num_runs=5):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    img_features = batch[\"img_features\"].to(device)\n",
    "    text_features = batch[\"text_features\"].to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(img_features, text_features)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(img_features, text_features)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n",
    "# Benchmark on GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Benchmarking on GPU...\")\n",
    "    classifier_gpu = classifier  # Already on GPU\n",
    "    gpu_time = benchmark_inference(classifier_gpu, test_feature_loader, torch.device(\"cuda\"))\n",
    "    print(f\"GPU inference time: {gpu_time * 1000:.2f} ms per batch\")\n",
    "    \n",
    "    # Benchmark on CPU\n",
    "    print(\"\\nBenchmarking on CPU...\")\n",
    "    classifier_cpu = MultiModalClassifier(RetailProductFusionModel(\n",
    "        fusion_type=MODEL_CONFIG[\"fusion_type\"],\n",
    "        img_feature_dim=MODEL_CONFIG[\"img_feature_dim\"],\n",
    "        text_feature_dim=MODEL_CONFIG[\"text_feature_dim\"],\n",
    "        hidden_dim=MODEL_CONFIG[\"hidden_dim\"],\n",
    "        output_dim=MODEL_CONFIG[\"output_dim\"]\n",
    "    ), num_classes)\n",
    "    classifier_cpu.load_state_dict(classifier.state_dict())\n",
    "    classifier_cpu = classifier_cpu.to(torch.device(\"cpu\"))\n",
    "    \n",
    "    cpu_time = benchmark_inference(classifier_cpu, test_feature_loader, torch.device(\"cpu\"))\n",
    "    print(f\"CPU inference time: {cpu_time * 1000:.2f} ms per batch\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nGPU speedup: {speedup:.2f}x faster than CPU\")\n",
    "else:\n",
    "    print(\"GPU not available for benchmarking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the performance difference between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    platforms = ['CPU', 'GPU']\n",
    "    times = [cpu_time * 1000, gpu_time * 1000]  # Convert to ms\n",
    "    \n",
    "    bars = plt.bar(platforms, times, color=['lightgray', 'green'])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.2f} ms', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title('Inference Time Comparison: CPU vs. GPU', fontsize=14)\n",
    "    plt.ylabel('Time per batch (ms)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add speedup text\n",
    "    plt.figtext(0.5, 0.01, f\"GPU Speedup: {speedup:.2f}x\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export and Integration\n",
    "\n",
    "Finally, let's export our model for integration into the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export model metadata\n",
    "model_metadata = {\n",
    "    \"model_type\": \"MultiModalClassifier\",\n",
    "    \"fusion_type\": MODEL_CONFIG[\"fusion_type\"],\n",
    "    \"img_feature_dim\": MODEL_CONFIG[\"img_feature_dim\"],\n",
    "    \"text_feature_dim\": MODEL_CONFIG[\"text_feature_dim\"],\n",
    "    \"hidden_dim\": MODEL_CONFIG[\"hidden_dim\"],\n",
    "    \"output_dim\": MODEL_CONFIG[\"output_dim\"],\n",
    "    \"num_classes\": num_classes,\n",
    "    \"category_mapping\": idx_to_category,\n",
    "    \"test_accuracy\": test_acc,\n",
    "    \"trained_on\": \"product_catalog_dataset\",\n",
    "    \"date_trained\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(REPO_ROOT / \"models\" / \"multimodal_model_metadata.json\", 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "\n",
    "print(\"Model exported successfully with metadata.\")\n",
    "print(f\"Model location: {REPO_ROOT / 'models' / 'best_multimodal_classifier.pth'}\")\n",
    "print(f\"Metadata location: {REPO_ROOT / 'models' / 'multimodal_model_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Loaded pre-trained vision and language models\n",
    "2. Created a multi-modal dataset for retail product classification\n",
    "3. Built and trained a multi-modal fusion model\n",
    "4. Compared the multi-modal model with single-modal baselines\n",
    "5. Analyzed the performance benefits of GPU acceleration\n",
    "6. Exported the model for integration into our inference pipeline\n",
    "\n",
    "Our multi-modal approach demonstrated superior performance compared to single-modal approaches, highlighting the value of combining multiple data modalities for retail applications. NVIDIA GPU acceleration provided significant performance improvements, enabling faster training and inference.\n",
    "\n",
    "In the next notebook, we'll build an end-to-end inference pipeline that integrates this model for real-time retail applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
